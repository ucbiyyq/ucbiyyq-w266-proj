{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "scatch to build the final notebook\n",
    "\n",
    "this version uses GloVe for pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import janitor\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_interim_data(p_ca_pos, p_out):\n",
    "    '''\n",
    "    helper function to prepare the interim dataset\n",
    "    intention is to mimic the catalog data at the Employer\n",
    "    we don't need much of the data, just the unspsc codes and the item names\n",
    "    \n",
    "    params\n",
    "        p_ca_pos\n",
    "            path to the data file, i.e.\n",
    "            ~/data/external/ca-po/PURCHASE_ORDER_DATA_EXTRACT_2012-2015_CLEANED.zip\n",
    "        p_unspsc_items\n",
    "            path to the iterim data file, i.e.\n",
    "            ~/data/interim/unspsc-item.csv\n",
    "    '''\n",
    "    t1 = pd.read_csv(p_ca_pos, dtype=str).clean_names()\n",
    "    t2 = t1[[\"normalized_unspsc\", \"item_name\"]]\n",
    "    t3 = t2.drop_duplicates().dropna()\n",
    "    t4 = t3.sort_values(by=[\"normalized_unspsc\", \"item_name\"]).reset_index(drop = True)\n",
    "    t4.to_csv(p_out, index = False)\n",
    "    print(\"interim data saved to \", p_unspsc_items)\n",
    "    \n",
    "def sample_unspsc_item(unspsc_items, n_unspsc, n_per_unspsc, min_per_unspsc, max_per_unspsc, random_state = 0):\n",
    "    '''\n",
    "    helper function to sample n_per_unspsc number of examples per unspsc code,\n",
    "    and n_unspsc number of codes overall\n",
    "    \n",
    "    params\n",
    "        unspsc_items\n",
    "            the data frame that contains the unspsc_items\n",
    "        n_unspsc\n",
    "            the number of unspsc codes we want to use in our run\n",
    "        n_per_unspsc\n",
    "            the number of items we want per unspsc code\n",
    "            should be between min_per_unspsc and max_per_unspsc\n",
    "        min_per_unspsc\n",
    "            the min number of items we want per class \n",
    "        max_per_unspsc\n",
    "            the max number of items we want per class\n",
    "        random_state\n",
    "            to set random seed, default 0\n",
    "    '''\n",
    "    # gets count of items per unspsc code, \n",
    "    # and filters for codes that have number of items between the min and max\n",
    "    t1 = unspsc_items.groupby(\"normalized_unspsc\").count().reset_index()\n",
    "    t2 = t1.item_name.between(min_per_unspsc, max_per_unspsc, inclusive = True)\n",
    "    t3 = t1[t2]\n",
    "    relevant = t3\n",
    "    \n",
    "    # gets list of unique unspsc codes,\n",
    "    # and randomly picks n_unspsc number of them for use\n",
    "    t1 = relevant[[\"normalized_unspsc\"]]\n",
    "    t2 = t1.drop_duplicates()\n",
    "    t3 = t2.sample(n = n_unspsc, random_state = random_state).normalized_unspsc\n",
    "    uc = t3\n",
    "    \n",
    "    # gets only the items for the randomly picked unspsc codes\n",
    "    filtered = unspsc_items[unspsc_items[\"normalized_unspsc\"].isin(uc)]\n",
    "    \n",
    "    # gets a sample of items from each randomly picked unspsc code\n",
    "    t1 = filtered.groupby(\"normalized_unspsc\")\n",
    "    t2 = t1.apply(lambda x: x.sample(n = n_per_unspsc, replace = False, random_state = random_state))\n",
    "    t3 = t2.reset_index(drop  = True).sort_values(by = [\"normalized_unspsc\", \"item_name\"]).reset_index(drop  = True)\n",
    "    t4 = t3.drop_duplicates()\n",
    "    results = t4\n",
    "    return(results)\n",
    "\n",
    "def get_embeddings(p_glove):\n",
    "    '''\n",
    "    helper function to load GloVe embeddings\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    \n",
    "    params\n",
    "        p_glove\n",
    "            path to the GloVe embeddings, i.e.\n",
    "            ~/data/external/glove/glove.6B.100d.txt\n",
    "    '''\n",
    "    embeddings_index = {}\n",
    "    with open(p_glove) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    return(embeddings_index)\n",
    "\n",
    "def prep_text_and_labels(unspsc_item):\n",
    "    '''\n",
    "    helper function to convert unspsc_item_samples into format usable for training\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    \n",
    "    params\n",
    "        unspsc_item\n",
    "            dataframe of unspsc_item, or some sample of unspsc_item\n",
    "    '''\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "    \n",
    "    texts = unspsc_item.item_name.tolist()\n",
    "    label_id = 0\n",
    "    for x in unspsc_item.normalized_unspsc:\n",
    "        if x in labels_index.keys():\n",
    "            pass\n",
    "        else:\n",
    "            label_id = len(labels_index)\n",
    "        labels_index[x] = label_id\n",
    "        labels.append(label_id)\n",
    "    return(texts, labels_index, labels)\n",
    "\n",
    "def vectorize_texts(texts, labels, n_max_words, n_padding):\n",
    "    '''\n",
    "    helper function to vectorize the text samples into a 2D integer tensor\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    \n",
    "    params\n",
    "        texts\n",
    "            list of texts\n",
    "        labels\n",
    "            list of class labels\n",
    "    '''\n",
    "    tokenizer = Tokenizer(num_words = n_max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen = n_padding, padding = \"post\")\n",
    "\n",
    "    labels_oh = to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels_oh.shape)\n",
    "    \n",
    "    return(data, labels_oh, word_index)\n",
    "\n",
    "def prep_embedding_matrix(word_index, embeddings_index, n_max_words, embed_dim):\n",
    "    '''\n",
    "    helper function to prepare the embedding matrix\n",
    "    basically looks up the word in the embeddings index created earlier from GloVe\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    \n",
    "    params\n",
    "        word_index\n",
    "        embeddings_index\n",
    "        n_max_words\n",
    "        embed_dim\n",
    "    '''\n",
    "    num_words = min(n_max_words, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= n_max_words:\n",
    "            # ignores words that are more than the max number of words allowed\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to data\n",
    "p_ca_pos = os.path.join(\"..\", \"data\", \"external\", \"ca-po\", \"PURCHASE_ORDER_DATA_EXTRACT_2012-2015_CLEANED.zip\")\n",
    "p_unspsc_items = os.path.join(\"..\", \"data\", \"interim\", \"unspsc-item.csv\")\n",
    "p_glove = os.path.join(\"..\", \"data\", \"external\", \"glove\", \"glove.6B.100d.txt\")\n",
    "\n",
    "# some global params\n",
    "NUM_CLASSES = 10 # number of classes we want to try for in this run\n",
    "NUM_EXAMPLES_PER_CLASS = 3 # number of instances we want per class, to simulate catalog data, should be between min and max\n",
    "MIN_EXAMPLES_PER_CLASS = 3 # for picking good examples, should be greater than 1\n",
    "MAX_EXAMPLES_PER_CLASS = 99999 # for picking good examples, should be greater than MIN_EXAMPLES_PER_CLASS\n",
    "\n",
    "MAX_NUM_WORDS = 2000 # for vocab size\n",
    "MAX_SEQUENCE_LENGTH = 1000 # for padding\n",
    "VALIDATION_SPLIT = 0.5 # for train vs validation set size (this is per class split)\n",
    "EMBEDDING_DIM = 100 # i.e. how wide to make the embedding. Since we are using GloVe, keep at 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Inspired by problem from work. Global procurement system. Every item needs to be categorized for catalog. Item to a code. By nature of a catalog, very few items belong in each code \"bucket\". And there are lots of items and lots of codes, so looking for a better way to categorize items.\n",
    "\n",
    "We don't have access to the real data from work. Instead we start with the California PO data. As part of POs, there are items and unspsc codes, which can stand in for the catalog data from work. We will need to massage the data a bit to make it resemble work data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interim data saved to  ../data/interim/unspsc-item.csv\n"
     ]
    }
   ],
   "source": [
    "# gets and preps interim data\n",
    "# We don't need all the PO data, so just stripping it down to item and unspsc codes\n",
    "prep_interim_data(p_ca_pos, p_unspsc_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets interim data\n",
    "unspsc_items = pd.read_csv(p_unspsc_items, dtype=str).clean_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to get a sample of the unspsc codes and items\n",
    "\n",
    "even if unbalanced, to mimic catalogs, we would want only a couple of items per unspsc code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note most unspsc codes only have a few items, which is quite similar to the real data that we don't have access to. Average items per code is 3. To make the California data reflect the real data, we need to constrain our data so that we only have a few items per unspsc code. But for the train-validate split to work, we need at least two items per code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median items per unspsc code 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1 = unspsc_items.groupby(\"normalized_unspsc\").count()\n",
    "t2 = t1.item_name.tolist()\n",
    "print(\"median items per unspsc code\", statistics.median(t2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "t1.hist(ax=ax, bins=100, bottom=0.1)\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preps samples\n",
    "unspsc_item_samples = sample_unspsc_item(\n",
    "    unspsc_items, \n",
    "    n_unspsc = NUM_CLASSES, \n",
    "    n_per_unspsc = NUM_EXAMPLES_PER_CLASS,\n",
    "    min_per_unspsc = MIN_EXAMPLES_PER_CLASS,\n",
    "    max_per_unspsc = MAX_EXAMPLES_PER_CLASS, \n",
    "    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_item_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_unspsc</th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20122105</td>\n",
       "      <td>FIRING, PURPLE SHOT #6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20122105</td>\n",
       "      <td>campfire rings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20122105</td>\n",
       "      <td>heads green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20122344</td>\n",
       "      <td>Bolt  (0370.0577.1B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20122344</td>\n",
       "      <td>Clamp Plate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20122344</td>\n",
       "      <td>spacer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20122601</td>\n",
       "      <td>CA Integrated Seismic Network (CISN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20122601</td>\n",
       "      <td>California Strong Motion Instrumentation Program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20122601</td>\n",
       "      <td>Iteraction of MSE Autments with superstructure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20142710</td>\n",
       "      <td>Bearing Assembly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20142710</td>\n",
       "      <td>File Bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20142710</td>\n",
       "      <td>under shelf closet hanger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   normalized_unspsc                                          item_name\n",
       "0           20122105                             FIRING, PURPLE SHOT #6\n",
       "1           20122105                                     campfire rings\n",
       "2           20122105                                        heads green\n",
       "3           20122344                               Bolt  (0370.0577.1B)\n",
       "4           20122344                                        Clamp Plate\n",
       "5           20122344                                             spacer\n",
       "6           20122601               CA Integrated Seismic Network (CISN)\n",
       "7           20122601   California Strong Motion Instrumentation Program\n",
       "8           20122601  Iteraction of MSE Autments with superstructure...\n",
       "9           20142710                                   Bearing Assembly\n",
       "10          20142710                                           File Bar\n",
       "11          20142710                          under shelf closet hanger"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_item_samples.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_unspsc</th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>52121601</td>\n",
       "      <td>Linen supply rental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       normalized_unspsc            item_name\n",
       "count                 30                   30\n",
       "unique                10                   30\n",
       "top             52121601  Linen supply rental\n",
       "freq                   3                    1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_item_samples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalized_unspsc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20122105</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20122344</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20122601</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20142710</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30111601</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31161833</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39121574</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47121602</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52121601</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80141502</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   item_name\n",
       "normalized_unspsc           \n",
       "20122105                   3\n",
       "20122344                   3\n",
       "20122601                   3\n",
       "20142710                   3\n",
       "30111601                   3\n",
       "31161833                   3\n",
       "39121574                   3\n",
       "47121602                   3\n",
       "52121601                   3\n",
       "80141502                   3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_item_samples.groupby(\"normalized_unspsc\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels_index, labels = prep_text_and_labels(unspsc_item_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 unique tokens.\n",
      "Shape of data tensor: (30, 1000)\n",
      "Shape of label tensor: (30, 10)\n"
     ]
    }
   ],
   "source": [
    "# note, the labels gets converted to a one-hot class matrix, \n",
    "# so that we later we can calculate cross-entropy loss against it\n",
    "data, labels_oh, word_index = vectorize_texts(texts, labels, MAX_NUM_WORDS, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(101, 100)\n",
      "number of words  101\n",
      "<keras.layers.embeddings.Embedding object at 0x7f74649b6ba8>\n"
     ]
    }
   ],
   "source": [
    "# get embeddings\n",
    "embeddings_index = get_embeddings(p_glove)\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# get embedding matrix\n",
    "embedding_matrix = prep_embedding_matrix(\n",
    "    word_index = word_index, \n",
    "    embeddings_index = embeddings_index, \n",
    "    n_max_words = MAX_NUM_WORDS, \n",
    "    embed_dim = EMBEDDING_DIM)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# as per the tutorial, we set trainable = False so as to keep the embeddings fixed\n",
    "# this makes sense because we are using the GloVe embeddings, and are not training our own embeddings\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    embeddings_initializer = Constant(embedding_matrix),\n",
    "    input_length = MAX_SEQUENCE_LENGTH,\n",
    "    trainable = False)\n",
    "\n",
    "print(\"number of words \", num_words)\n",
    "print(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN with global maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yangyq/anaconda3/envs/w266/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/yangyq/anaconda3/envs/w266/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/yangyq/anaconda3/envs/w266/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 20 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2.3098 - acc: 0.2000 - val_loss: 2.2655 - val_acc: 0.2000\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 2.2176 - acc: 0.5500 - val_loss: 2.2249 - val_acc: 0.2000\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 2.0675 - acc: 0.2500 - val_loss: 2.1527 - val_acc: 0.2000\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 1.8623 - acc: 0.5500 - val_loss: 2.0415 - val_acc: 0.2000\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 1.5936 - acc: 0.7500 - val_loss: 2.0090 - val_acc: 0.3000\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 1.3227 - acc: 0.8000 - val_loss: 2.0130 - val_acc: 0.3000\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 1.1908 - acc: 0.7500 - val_loss: 2.1951 - val_acc: 0.4000\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 1.1816 - acc: 0.7500 - val_loss: 2.0710 - val_acc: 0.2000\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.8793 - acc: 0.8500 - val_loss: 1.8090 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.6508 - acc: 1.0000 - val_loss: 1.8501 - val_acc: 0.3000\n"
     ]
    }
   ],
   "source": [
    "def g1(\n",
    "    data, \n",
    "    labels, \n",
    "    labels_oh, \n",
    "    labels_index, \n",
    "    embedding_layer,\n",
    "    split,\n",
    "    seq_shape):\n",
    "    '''\n",
    "    1D CNN, because we're dealing with short text phrases, not images, 1D instead of 2D CNN\n",
    "    also, item names don't exactly follow English grammar rules\n",
    "    \n",
    "    params\n",
    "        data\n",
    "            matrix of vectorized text data\n",
    "        labels\n",
    "            array of label ids for each of the texts\n",
    "        labels_oh\n",
    "            one-hot matrix of labels for each of the texts\n",
    "        labels_index\n",
    "            dictionary of labels and ids\n",
    "        embedding_layer\n",
    "            the frozen embedding layer we defined with GloVe\n",
    "        split\n",
    "            float that represnets the validation split size\n",
    "        seq_shape\n",
    "            sequence length, i.e. the length with padding\n",
    "    \n",
    "    see https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    '''\n",
    "    \n",
    "    def split_data(data, labels, labels_oh, split, random_state = 0):\n",
    "        '''\n",
    "        split the data into a training set and a validation set\n",
    "        note for this to simulate catalog data, we need stratified split, per class\n",
    "\n",
    "        params\n",
    "            data\n",
    "                matrix of vectorized text data\n",
    "            labels\n",
    "                array of label ids for each of the texts\n",
    "            labels_oh\n",
    "                one-hot matrix of labels for each of the texts\n",
    "            split\n",
    "                float that represnets the validation split size\n",
    "        '''\n",
    "        np.random.seed(random_state)\n",
    "        x_train = np.empty((0, data[0].shape[0]))\n",
    "        y_train = np.empty((0, labels_oh[0].shape[0]))\n",
    "        x_val = np.empty((0, data[0].shape[0]))\n",
    "        y_val = np.empty((0, labels_oh[0].shape[0]))\n",
    "\n",
    "        # gets unique classes\n",
    "        uc = np.unique(labels)\n",
    "\n",
    "        # for each class, \n",
    "        # subsets the data and labels_oh, \n",
    "        # and assigns some to train, some to validation \n",
    "        # this makes sure each class has some representation in train, and some in validation\n",
    "        for c in uc:\n",
    "            t1 = (labels == c)\n",
    "            data_c = data[t1]\n",
    "            labels_oh_c = labels_oh[t1]\n",
    "            indices = np.arange(data_c.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            data_c = data_c[indices]\n",
    "            labels_oh_c = labels_oh_c[indices]\n",
    "            num_validation_samples = int(split * data_c.shape[0])\n",
    "            x_train_c = data_c[:-num_validation_samples]\n",
    "            y_train_c = labels_oh_c[:-num_validation_samples]\n",
    "            x_val_c = data_c[-num_validation_samples:]\n",
    "            y_val_c = labels_oh_c[-num_validation_samples:]\n",
    "\n",
    "            x_train = np.concatenate([x_train, x_train_c])\n",
    "            y_train = np.concatenate([y_train, y_train_c])\n",
    "            x_val = np.concatenate([x_val, x_val_c])\n",
    "            y_val = np.concatenate([y_val, y_val_c])\n",
    "\n",
    "        return(x_train, y_train, x_val, y_val)\n",
    "    \n",
    "    # split the vectorized data into train and validation sets\n",
    "    x_train, y_train, x_val, y_val = split_data(data, labels, labels_oh, split)\n",
    "    \n",
    "    # 1D CNN\n",
    "    sequence_input = Input(shape = (seq_shape,), dtype = \"int32\")\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 5, activation = \"relu\")(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation = \"relu\")(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation = \"relu\")(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation = \"relu\")(x)\n",
    "    preds = Dense(len(labels_index), activation = \"softmax\")(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss = \"categorical_crossentropy\",\n",
    "                  optimizer = \"rmsprop\",\n",
    "                  metrics=[\"acc\"])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              validation_data=(x_val, y_val))\n",
    "    \n",
    "g1( data = data, \n",
    "    labels = labels, \n",
    "    labels_oh = labels_oh, \n",
    "    labels_index = labels_index, \n",
    "    embedding_layer = embedding_layer,\n",
    "    split = VALIDATION_SPLIT,\n",
    "    seq_shape = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about siamese?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer flatten_4: expected min_ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-8627e6489d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVALIDATION_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     seq_shape = MAX_SEQUENCE_LENGTH)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-8627e6489d8f>\u001b[0m in \u001b[0;36mg2\u001b[0;34m(data, labels, embedding_layer, split, seq_shape)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# network definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mbase_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_base_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0minput_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-8627e6489d8f>\u001b[0m in \u001b[0;36mcreate_base_network\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m    105\u001b[0m         '''\n\u001b[1;32m    106\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected min_ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer flatten_4: expected min_ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "def g2(\n",
    "    data, \n",
    "    labels, \n",
    "    embedding_layer,\n",
    "    split,\n",
    "    seq_shape):\n",
    "    '''\n",
    "    basic siamese network\n",
    "    \n",
    "    params\n",
    "        data\n",
    "        labels\n",
    "        embedding_layer\n",
    "        split\n",
    "        seq_shape\n",
    "    \n",
    "    see\n",
    "    https://keras.io/examples/mnist_siamese/\n",
    "    https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py\n",
    "    https://sorenbouma.github.io/blog/oneshot/\n",
    "    https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d\n",
    "    https://github.com/sorenbouma/keras-oneshot/blob/master/SiameseNet.ipynb\n",
    "    '''\n",
    "    \n",
    "    def split_data(data, labels, split, random_state = 0):\n",
    "        '''\n",
    "        split the data into a training set and a validation set\n",
    "        note for this to simulate catalog data, we need stratified split, per class\n",
    "\n",
    "        params\n",
    "            data\n",
    "                matrix of vectorized text data\n",
    "            labels\n",
    "                array of label ids for each of the texts\n",
    "            labels_oh\n",
    "                one-hot matrix of labels for each of the texts\n",
    "            split\n",
    "                float that represnets the validation split size\n",
    "        '''\n",
    "        np.random.seed(random_state)\n",
    "        x_train = np.empty((0, data[0].shape[0]))\n",
    "        y_train = np.empty((0, ))\n",
    "        x_val = np.empty((0, data[0].shape[0]))\n",
    "        y_val = np.empty((0, ))\n",
    "\n",
    "        # gets unique classes\n",
    "        uc = np.unique(labels)\n",
    "\n",
    "        # for each class, \n",
    "        # subsets the data and labels_oh, \n",
    "        # and assigns some to train, some to validation \n",
    "        # this makes sure each class has some representation in train, and some in validation\n",
    "        for c in uc:\n",
    "            t1 = (labels == c)\n",
    "            t2 = np.array(labels)\n",
    "            data_c = data[t1]\n",
    "            labels_c = t2[t1]\n",
    "            indices = np.arange(data_c.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            data_c = data_c[indices]\n",
    "            labels_c = labels_c[indices]\n",
    "            num_validation_samples = int(split * data_c.shape[0])\n",
    "            x_train_c = data_c[:-num_validation_samples]\n",
    "            y_train_c = labels_c[:-num_validation_samples]\n",
    "            x_val_c = data_c[-num_validation_samples:]\n",
    "            y_val_c = labels_c[-num_validation_samples:]\n",
    "            \n",
    "            x_train = np.concatenate([x_train, x_train_c])\n",
    "            y_train = np.concatenate([y_train, y_train_c])\n",
    "            x_val = np.concatenate([x_val, x_val_c])\n",
    "            y_val = np.concatenate([y_val, y_val_c])\n",
    "\n",
    "        return(x_train, y_train, x_val, y_val)\n",
    "    \n",
    "    def create_pairs(x, unspsc_indices, num_classes):\n",
    "        '''\n",
    "        Positive and negative pair creation.\n",
    "        Alternates between positive and negative pairs.\n",
    "\n",
    "        params\n",
    "            x\n",
    "            unspsc_indices\n",
    "            num_classes\n",
    "\n",
    "        see https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py\n",
    "        '''\n",
    "        pairs = []\n",
    "        labels = []\n",
    "        n = min([len(unspsc_indices[d]) for d in range(num_classes)]) - 1\n",
    "        for d in range(num_classes):\n",
    "            for i in range(n):\n",
    "                z1, z2 = unspsc_indices[d][i], unspsc_indices[d][i + 1]\n",
    "                pairs += [[x[z1], x[z2]]]\n",
    "                inc = random.randrange(1, num_classes)\n",
    "                dn = (d + inc) % num_classes\n",
    "                z1, z2 = unspsc_indices[d][i], unspsc_indices[dn][i]\n",
    "                pairs += [[x[z1], x[z2]]]\n",
    "                labels += [1, 0]\n",
    "        return np.array(pairs), np.array(labels)\n",
    "    \n",
    "    def create_base_network(input_shape):\n",
    "        '''\n",
    "        Base network to be shared (eq. to feature extraction).\n",
    "        This is what makes the network \"siamese\"\n",
    "        '''\n",
    "        input = Input(shape = input_shape)\n",
    "        x = Flatten()(input)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        return Model(input, x)\n",
    "\n",
    "    \n",
    "    # split the vectorized data into train and validation sets\n",
    "    x_train, y_train, x_val, y_val = split_data(data, labels, split)\n",
    "    input_shape = x_train.shape[1:]\n",
    "    \n",
    "    # gets unique classes\n",
    "    num_classes = len(np.unique(labels))\n",
    "    \n",
    "    # creates the pairs\n",
    "    unspsc_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\n",
    "    tr_pairs, tr_y = create_pairs(x_train, unspsc_indices, num_classes)\n",
    "\n",
    "    indices = [np.where(y_val == i)[0] for i in range(num_classes)]\n",
    "    te_pairs, te_y = create_pairs(x_val, indices, num_classes)\n",
    "    \n",
    "    # network definition\n",
    "    base_network = create_base_network(input_shape)\n",
    "\n",
    "    input_a = Input(shape = input_shape)\n",
    "    input_b = Input(shape = input_shape)\n",
    "    \n",
    "    # because we re-use the same instance `base_network`,\n",
    "    # the weights of the network\n",
    "    # will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "\n",
    "    distance = Lambda(euclidean_distance, output_shape = eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "    model = Model([input_a, input_b], distance)\n",
    "    \n",
    "    #\n",
    "#     sequence_input = Input(shape = (seq_shape,), dtype = \"int32\")\n",
    "#     embedded_sequences = embedding_layer(sequence_input)\n",
    "#     print(embedded_sequences)\n",
    "\n",
    "g2( data = data, \n",
    "    labels = labels, \n",
    "    embedding_layer = embedding_layer,\n",
    "    split = VALIDATION_SPLIT,\n",
    "    seq_shape = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations & Future Work\n",
    "\n",
    "Use character level embeddings. GloVe works better at sentences, with real words. But item names or descriptions not necessarily actual sentences.\n",
    "\n",
    "GloVe assumes standard english words, but certain items names might not have standard tokens.\n",
    "\n",
    "This is vulnerable to items being thrown into the most permissive category. Will tend to select for those categories since they will have the most item names. Less appetizing for business use, since often the business aim of categorization exercises is to build a better balanced categorization that is more expressive and can be used for downstream analytics. Maybe this approach can be complemented by negatively weighing the more popular categories.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

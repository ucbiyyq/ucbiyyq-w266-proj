{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "scatch to build the final notebook\n",
    "\n",
    "this version uses GloVe for pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import janitor\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_interim_data(p_ca_pos, p_out):\n",
    "    '''\n",
    "    helper function to prepare the interim dataset\n",
    "    intention is to mimic the catalog data at the Employer\n",
    "    we don't need much of the data, just the unspsc codes and the item names\n",
    "    \n",
    "    params\n",
    "        p_ca_pos\n",
    "            path to the data file, i.e.\n",
    "            ~/data/external/ca-po/PURCHASE_ORDER_DATA_EXTRACT_2012-2015_CLEANED.zip\n",
    "        \n",
    "        p_unspsc_items\n",
    "            path to the iterim data file, i.e.\n",
    "            ~/data/interim/unspsc-item.csv\n",
    "            \n",
    "    '''\n",
    "    t1 = pd.read_csv(p_ca_pos, dtype=str).clean_names()\n",
    "    t2 = t1[[\"normalized_unspsc\", \"item_name\"]]\n",
    "    t3 = t2.drop_duplicates().dropna()\n",
    "    t4 = t3.sort_values(by=[\"normalized_unspsc\", \"item_name\"]).reset_index(drop = True)\n",
    "    t4.to_csv(p_out, index = False)\n",
    "    print(\"interim data saved to \", p_unspsc_items)\n",
    "    \n",
    "def sample_unspsc_item(unspsc_items, n_unspsc, n_per_unspsc, min_per_unspsc, max_per_unspsc, random_state = 0):\n",
    "    '''\n",
    "    helper function to sample n_per_unspsc number of examples per unspsc code,\n",
    "    and n_unspsc number of codes overall\n",
    "    \n",
    "    params\n",
    "        unspsc_items\n",
    "            the data frame that contains the unspsc_items\n",
    "            \n",
    "        n_unspsc\n",
    "            the number of unspsc codes we want to use in our run\n",
    "        \n",
    "        n_per_unspsc\n",
    "            the number of items we want per unspsc code\n",
    "            should be between min_per_unspsc and max_per_unspsc\n",
    "        \n",
    "        min_per_unspsc\n",
    "            the min number of items we want per class \n",
    "            \n",
    "        max_per_unspsc\n",
    "            the max number of items we want per class\n",
    "        \n",
    "        random_state\n",
    "            to set random seed, default 0\n",
    "    '''\n",
    "    # gets count of items per unspsc code, \n",
    "    # and filters for codes that have number of items between the min and max\n",
    "    t1 = unspsc_items.groupby(\"normalized_unspsc\").count().reset_index()\n",
    "    t2 = t1.item_name.between(min_per_unspsc, max_per_unspsc, inclusive = True)\n",
    "    t3 = t1[t2]\n",
    "    relevant = t3\n",
    "    \n",
    "    # gets list of unique unspsc codes,\n",
    "    # and randomly picks n_unspsc number of them for use\n",
    "    t1 = relevant[[\"normalized_unspsc\"]]\n",
    "    t2 = t1.drop_duplicates()\n",
    "    t3 = t2.sample(n = n_unspsc, random_state = random_state).normalized_unspsc\n",
    "    uc = t3\n",
    "    \n",
    "    # gets only the items for the randomly picked unspsc codes\n",
    "    filtered = unspsc_items[unspsc_items[\"normalized_unspsc\"].isin(uc)]\n",
    "    \n",
    "    # gets a sample of items from each randomly picked unspsc code\n",
    "    t1 = filtered.groupby(\"normalized_unspsc\")\n",
    "    t2 = t1.apply(lambda x: x.sample(n = n_per_unspsc, replace = False, random_state = random_state))\n",
    "    t3 = t2.reset_index(drop  = True).sort_values(by = [\"normalized_unspsc\", \"item_name\"]).reset_index(drop  = True)\n",
    "    t4 = t3.drop_duplicates()\n",
    "    results = t4\n",
    "    return(results)\n",
    "\n",
    "def get_embeddings(p_glove):\n",
    "    '''\n",
    "    helper function to load GloVe embeddings\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    \n",
    "    params\n",
    "        p_glove\n",
    "            path to the GloVe embeddings, i.e.\n",
    "            ~/data/external/glove/glove.6B.100d.txt\n",
    "    '''\n",
    "    embeddings_index = {}\n",
    "    with open(p_glove) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    return(embeddings_index)\n",
    "\n",
    "def prep_text_and_labels(unspsc_item):\n",
    "    '''\n",
    "    helper function to convert unspsc_item_samples into format usable for training\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    \n",
    "    params\n",
    "        unspsc_item\n",
    "            dataframe of unspsc_item, or some sample of unspsc_item\n",
    "    '''\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "    \n",
    "    texts = unspsc_item.item_name.tolist()\n",
    "    label_id = 0\n",
    "    for x in unspsc_item.normalized_unspsc:\n",
    "        if x in labels_index.keys():\n",
    "            pass\n",
    "        else:\n",
    "            label_id = len(labels_index)\n",
    "        labels_index[x] = label_id\n",
    "        labels.append(label_id)\n",
    "    return(texts, labels_index, labels)\n",
    "\n",
    "def vectorize_texts(texts, labels, n_max_words, n_padding):\n",
    "    '''\n",
    "    helper function to vectorize the text samples into a 2D integer tensor\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    \n",
    "    params\n",
    "        texts\n",
    "            list of texts\n",
    "        \n",
    "        labels\n",
    "            list of class labels\n",
    "    '''\n",
    "    tokenizer = Tokenizer(num_words = n_max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen = n_padding, padding = \"post\")\n",
    "\n",
    "    labels_oh = to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels_oh.shape)\n",
    "    \n",
    "    return(data, labels_oh, word_index)\n",
    "\n",
    "def prep_embedding_matrix(word_index, embeddings_index, n_max_words, embed_dim):\n",
    "    '''\n",
    "    helper function to prepare the embedding matrix\n",
    "    basically looks up the word in the embeddings index created earlier from GloVe\n",
    "    see https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "    \n",
    "    params\n",
    "        word_index\n",
    "        embeddings_index\n",
    "        n_max_words\n",
    "        embed_dim\n",
    "    '''\n",
    "    num_words = min(n_max_words, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= n_max_words:\n",
    "            # ignores words that are more than the max number of words allowed\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return(embedding_matrix)\n",
    "\n",
    "def split_data(data, labels, labels_oh, split, random_state = 0):\n",
    "    '''\n",
    "    split the data into a training set and a validation set\n",
    "    note for this to simulate catalog data, we need stratified split, per class\n",
    "    \n",
    "    params\n",
    "        data\n",
    "        labels\n",
    "        labels_oh\n",
    "        split\n",
    "    '''\n",
    "    np.random.seed(random_state)\n",
    "    x_train = np.empty((0, data[0].shape[0]))\n",
    "    y_train = np.empty((0, labels_oh[0].shape[0]))\n",
    "    x_val = np.empty((0, data[0].shape[0]))\n",
    "    y_val = np.empty((0, labels_oh[0].shape[0]))\n",
    "    \n",
    "    # gets unique classes\n",
    "    uc = np.unique(labels)\n",
    "    \n",
    "    # for each class, \n",
    "    # subsets the data and labels_oh, \n",
    "    # and assigns some to train, some to validation \n",
    "    # this makes sure each class has some representation in train, and some in validation\n",
    "    for c in uc:\n",
    "        t1 = (labels == c)\n",
    "        data_c = data[t1]\n",
    "        labels_oh_c = labels_oh[t1]\n",
    "        indices = np.arange(data_c.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        data_c = data_c[indices]\n",
    "        labels_oh_c = labels_oh_c[indices]\n",
    "        num_validation_samples = int(split * data_c.shape[0])\n",
    "        x_train_c = data_c[:-num_validation_samples]\n",
    "        y_train_c = labels_oh_c[:-num_validation_samples]\n",
    "        x_val_c = data_c[-num_validation_samples:]\n",
    "        y_val_c = labels_oh_c[-num_validation_samples:]\n",
    "        \n",
    "        x_train = np.concatenate([x_train, x_train_c])\n",
    "        y_train = np.concatenate([y_train, y_train_c])\n",
    "        x_val = np.concatenate([x_val, x_val_c])\n",
    "        y_val = np.concatenate([y_val, y_val_c])\n",
    "    \n",
    "    return(x_train, y_train, x_val, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to data\n",
    "p_ca_pos = os.path.join(\"..\", \"data\", \"external\", \"ca-po\", \"PURCHASE_ORDER_DATA_EXTRACT_2012-2015_CLEANED.zip\")\n",
    "p_unspsc_items = os.path.join(\"..\", \"data\", \"interim\", \"unspsc-item.csv\")\n",
    "p_glove = os.path.join(\"..\", \"data\", \"external\", \"glove\", \"glove.6B.100d.txt\")\n",
    "\n",
    "# some global params\n",
    "NUM_CLASSES = 10 # number of classes we want to try for in this run\n",
    "NUM_EXAMPLES_PER_CLASS = 2 # number of instances we want per class, to simulate catalog data, should be between min and max\n",
    "MIN_EXAMPLES_PER_CLASS = 2 # for picking good examples, should be greater than 1\n",
    "MAX_EXAMPLES_PER_CLASS = 99999 # for picking good examples, should be greater than MIN_EXAMPLES_PER_CLASS\n",
    "\n",
    "MAX_NUM_WORDS = 2000 # for vocab size\n",
    "MAX_SEQUENCE_LENGTH = 1000 # for padding\n",
    "VALIDATION_SPLIT = 0.5 # for train vs validation set size (this is per class split)\n",
    "EMBEDDING_DIM = 100 # i.e. how wide to make the embedding. Since we are using GloVe, keep at 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Inspired by problem from work. Global procurement system. Every item needs to be categorized for catalog. Item to a code. By nature of a catalog, very few items belong in each code \"bucket\". And there are lots of items and lots of codes, so looking for a better way to categorize items.\n",
    "\n",
    "We don't have access to the real data from work. Instead we start with the California PO data. As part of POs, there are items and unspsc codes, which can stand in for the catalog data from work. We will need to massage the data a bit to make it resemble work data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interim data saved to  ../data/interim/unspsc-item.csv\n"
     ]
    }
   ],
   "source": [
    "# gets and preps interim data\n",
    "# We don't need all the PO data, so just stripping it down to item and unspsc codes\n",
    "prep_interim_data(p_ca_pos, p_unspsc_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets interim data\n",
    "unspsc_items = pd.read_csv(p_unspsc_items, dtype=str).clean_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_unspsc</th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10101501</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10101502</td>\n",
       "      <td>Amendment 1 - Option to Renew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10101502</td>\n",
       "      <td>CANINE, LAW ENFORCEMENT, NON-BITE, SEARCH DOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10101502</td>\n",
       "      <td>CANINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10101502</td>\n",
       "      <td>CANINES, NON-BITE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  normalized_unspsc                                      item_name\n",
       "0          10101501                                         iphone\n",
       "1          10101502                  Amendment 1 - Option to Renew\n",
       "2          10101502  CANINE, LAW ENFORCEMENT, NON-BITE, SEARCH DOG\n",
       "3          10101502                                        CANINES\n",
       "4          10101502                              CANINES, NON-BITE"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215140, 2)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_unspsc</th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>215140</td>\n",
       "      <td>215140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13405</td>\n",
       "      <td>178890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>44103103</td>\n",
       "      <td>Medical Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2717</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       normalized_unspsc         item_name\n",
       "count             215140            215140\n",
       "unique             13405            178890\n",
       "top             44103103  Medical Supplies\n",
       "freq                2717               769"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_items.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to get a sample of the unspsc codes and items\n",
    "\n",
    "even if unbalanced, to mimic catalogs, we would want only a couple of items per unspsc code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note most unspsc codes only have a few items, which is quite similar to the real data that we don't have access to. Average items per code is 3. To make the California data reflect the real data, we need to constrain our data so that we only have a few items per unspsc code. But for the train-validate split to work, we need at least two items per code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median items per unspsc code 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1 = unspsc_items.groupby(\"normalized_unspsc\").count()\n",
    "t2 = t1.item_name.tolist()\n",
    "print(\"median items per unspsc code\", statistics.median(t2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "t1.hist(ax=ax, bins=100, bottom=0.1)\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preps samples\n",
    "unspsc_item_samples = sample_unspsc_item(\n",
    "    unspsc_items, \n",
    "    n_unspsc = NUM_CLASSES, \n",
    "    n_per_unspsc = NUM_EXAMPLES_PER_CLASS,\n",
    "    min_per_unspsc = MIN_EXAMPLES_PER_CLASS,\n",
    "    max_per_unspsc = MAX_EXAMPLES_PER_CLASS, \n",
    "    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_item_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_unspsc</th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11162111</td>\n",
       "      <td>Mech Assy Pass Side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11162111</td>\n",
       "      <td>mesh door guard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14122103</td>\n",
       "      <td>CLIN #15, ROLLSTOCK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14122103</td>\n",
       "      <td>ROLL STOCK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25101502</td>\n",
       "      <td>GLAVAL BUS CUT AWAY CHASSIS E-450 GAS BUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25101502</td>\n",
       "      <td>Inmate Security Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30161602</td>\n",
       "      <td>C12E0037, Agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30161602</td>\n",
       "      <td>PANELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41113720</td>\n",
       "      <td>fiber optic equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>41113720</td>\n",
       "      <td>traffic equipment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  normalized_unspsc                                  item_name\n",
       "0          11162111                        Mech Assy Pass Side\n",
       "1          11162111                            mesh door guard\n",
       "2          14122103                        CLIN #15, ROLLSTOCK\n",
       "3          14122103                                 ROLL STOCK\n",
       "4          25101502  GLAVAL BUS CUT AWAY CHASSIS E-450 GAS BUS\n",
       "5          25101502                        Inmate Security Bus\n",
       "6          30161602                        C12E0037, Agreement\n",
       "7          30161602                                     PANELS\n",
       "8          41113720                      fiber optic equipment\n",
       "9          41113720                          traffic equipment"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_item_samples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_unspsc</th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>52161514</td>\n",
       "      <td>1 OZ PLASTIC SOUFFLE CUPS,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       normalized_unspsc                   item_name\n",
       "count                 50                          50\n",
       "unique                10                          50\n",
       "top             52161514  1 OZ PLASTIC SOUFFLE CUPS,\n",
       "freq                   5                           1"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unspsc_item_samples.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels_index, labels = prep_text_and_labels(unspsc_item_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['Mech Assy Pass Side', 'mesh door guard', 'CLIN #15, ROLLSTOCK', 'ROLL STOCK', 'GLAVAL BUS CUT AWAY CHASSIS E-450 GAS BUS', 'Inmate Security Bus', 'C12E0037, Agreement', 'PANELS', 'fiber optic equipment', 'traffic equipment', 'CLAS ON LINE CAPABILITIES -', 'Workforce Scheduler', 'BENCHORGANIZER,L-SHAPE,ESD-SAFE', 'Monthly planer refill for 2014', 'CSP-SQ ORDER', 'csp-sq order', 'EEI Curriculum', 'Laboratory Supplies', 'Pager Service (MH 3of3)', 'Pager Service (Medical 2of2) (CIW)']\n",
      "{'11162111': 0, '14122103': 1, '25101502': 2, '30161602': 3, '41113720': 4, '43231605': 5, '44111516': 6, '50303501': 7, '60106207': 8, '81161705': 9}\n",
      "[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(texts)\n",
    "print(labels_index)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 unique tokens.\n",
      "Shape of data tensor: (20, 1000)\n",
      "Shape of label tensor: (20, 10)\n"
     ]
    }
   ],
   "source": [
    "# note, the labels gets converted to a one-hot class matrix, \n",
    "# so that we later we can calculate cross-entropy loss against it\n",
    "data, labels_oh, word_index = vectorize_texts(texts, labels, MAX_NUM_WORDS, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  9 10 ...  0  0  0]\n",
      " [12 13 14 ...  0  0  0]\n",
      " [15 16 17 ...  0  0  0]\n",
      " ...\n",
      " [31  0  0 ...  0  0  0]\n",
      " [32 33  2 ...  0  0  0]\n",
      " [34  2  0 ...  0  0  0]]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "{'bus': 1, 'equipment': 2, 'csp': 3, 'sq': 4, 'order': 5, 'pager': 6, 'service': 7, 'mech': 8, 'assy': 9, 'pass': 10, 'side': 11, 'mesh': 12, 'door': 13, 'guard': 14, 'clin': 15, '15': 16, 'rollstock': 17, 'roll': 18, 'stock': 19, 'glaval': 20, 'cut': 21, 'away': 22, 'chassis': 23, 'e': 24, '450': 25, 'gas': 26, 'inmate': 27, 'security': 28, 'c12e0037': 29, 'agreement': 30, 'panels': 31, 'fiber': 32, 'optic': 33, 'traffic': 34, 'clas': 35, 'on': 36, 'line': 37, 'capabilities': 38, 'workforce': 39, 'scheduler': 40, 'benchorganizer': 41, 'l': 42, 'shape': 43, 'esd': 44, 'safe': 45, 'monthly': 46, 'planer': 47, 'refill': 48, 'for': 49, '2014': 50, 'eei': 51, 'curriculum': 52, 'laboratory': 53, 'supplies': 54, 'mh': 55, '3of3': 56, 'medical': 57, '2of2': 58, 'ciw': 59}\n"
     ]
    }
   ],
   "source": [
    "print(data[0:10])\n",
    "print(labels_oh[0:10])\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = split_data(data, labels, labels_oh, VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n",
      "(10, 10)\n",
      "(10, 1000)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# get embeddings\n",
    "embeddings_index = get_embeddings(p_glove)\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = prep_embedding_matrix(word_index, embeddings_index, MAX_NUM_WORDS, EMBEDDING_DIM)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# as per the tutorial, we set trainable = False so as to keep the embeddings fixed\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    embeddings_initializer = Constant(embedding_matrix),\n",
    "    input_length = MAX_SEQUENCE_LENGTH,\n",
    "    trainable = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a 1D convnet with global maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 2.3137 - acc: 0.0000e+00 - val_loss: 2.2838 - val_acc: 0.2000\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 2.1992 - acc: 0.4000 - val_loss: 2.2497 - val_acc: 0.2000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.9891 - acc: 0.5000 - val_loss: 2.2043 - val_acc: 0.3000\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.7220 - acc: 0.7000 - val_loss: 2.1412 - val_acc: 0.2000\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.4101 - acc: 0.9000 - val_loss: 2.1023 - val_acc: 0.2000\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.1762 - acc: 0.9000 - val_loss: 2.0798 - val_acc: 0.2000\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.9738 - acc: 1.0000 - val_loss: 2.0880 - val_acc: 0.4000\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.7689 - acc: 0.9000 - val_loss: 1.9503 - val_acc: 0.4000\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.5415 - acc: 1.0000 - val_loss: 1.9726 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.3680 - acc: 1.0000 - val_loss: 1.9445 - val_acc: 0.4000\n"
     ]
    }
   ],
   "source": [
    "def g1(embedding_layer, labels_index, seq_shape):\n",
    "    sequence_input = Input(shape = (seq_shape,), dtype = \"int32\")\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 5, activation = \"relu\")(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation = \"relu\")(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation = \"relu\")(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation = \"relu\")(x)\n",
    "    preds = Dense(len(labels_index), activation = \"softmax\")(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss = \"categorical_crossentropy\",\n",
    "                  optimizer = \"rmsprop\",\n",
    "                  metrics=[\"acc\"])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              validation_data=(x_val, y_val))\n",
    "    \n",
    "g1(embedding_layer, labels_index, MAX_SEQUENCE_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
